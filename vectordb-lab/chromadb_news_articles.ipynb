{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-title",
      "metadata": { "id": "cell-title" },
      "source": [
        "# ChromaDB + OpenAI â€” News Articles Vector Search\n",
        "\n",
        "This notebook walks through building a **semantic search system** over a news articles dataset using:\n",
        "- `polars` â€” fast DataFrame library for reading the CSV\n",
        "- `openai` â€” `text-embedding-3-small` to generate vector embeddings\n",
        "- `chromadb` â€” vector database to store, index, and query embeddings\n",
        "\n",
        "---\n",
        "\n",
        "### Pipeline Overview\n",
        "```\n",
        "CSV File (articles)\n",
        "      â”‚\n",
        "      â–¼\n",
        "Polars DataFrame  â”€â”€â–º extract article text\n",
        "      â”‚\n",
        "      â–¼\n",
        "OpenAI Embedding API  â”€â”€â–º text â†’ 1536-dim vectors\n",
        "      â”‚\n",
        "      â–¼\n",
        "ChromaDB Collection  â”€â”€â–º store vectors + metadata\n",
        "      â”‚\n",
        "      â–¼\n",
        "Query  â”€â”€â–º embed query â”€â”€â–º cosine search â”€â”€â–º top-k results\n",
        "```\n",
        "\n",
        "---\n",
        "**Dataset:** [BBC News Articles (Kaggle)](https://www.kaggle.com/datasets/hgultekin/bbcnewsarchive)  \n",
        "Download `archive.zip`, extract, and upload `articles.csv` to Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-step1-md",
      "metadata": { "id": "cell-step1-md" },
      "source": [
        "## Step 1 â€” Install Dependencies\n",
        "\n",
        "We need three libraries that are not pre-installed in Colab:\n",
        "- `polars` â€” faster alternative to pandas, especially for large CSVs\n",
        "- `chromadb` â€” local vector database (no server needed)\n",
        "- `openai` â€” OpenAI Python SDK for embedding API calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-install",
      "metadata": { "id": "cell-install" },
      "outputs": [],
      "source": [
        "!pip install polars chromadb openai --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-step2-md",
      "metadata": { "id": "cell-step2-md" },
      "source": [
        "## Step 2 â€” Set OpenAI API Key\n",
        "\n",
        "Store your API key securely using **Colab Secrets** (recommended):\n",
        "1. Click the ğŸ”‘ key icon in the left sidebar\n",
        "2. Add a new secret named `OPENAI_API_KEY`\n",
        "3. Paste your key as the value\n",
        "4. Toggle **Notebook access** ON\n",
        "\n",
        "This avoids hardcoding your key in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-apikey",
      "metadata": { "id": "cell-apikey" },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Quick sanity check â€” should print True if key is loaded\n",
        "print(\"API key loaded:\", OPENAI_API_KEY is not None and len(OPENAI_API_KEY) > 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-step3-md",
      "metadata": { "id": "cell-step3-md" },
      "source": [
        "## Step 3 â€” Load Dataset with Polars\n",
        "\n",
        "**Why Polars instead of Pandas?**  \n",
        "Polars is built in Rust and is significantly faster for reading and filtering large CSVs. The API is similar to pandas but uses lazy evaluation under the hood.\n",
        "\n",
        "`with_row_index(offset=1)` adds an `index` column starting at 1 â€” we use this to generate unique document IDs for ChromaDB (`ID1`, `ID2`, ...).\n",
        "\n",
        "**Note:** The BBC dataset uses `iso-8859-1` encoding (Latin-1), not UTF-8. Without specifying this, Polars throws a decoding error on special characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-load-data",
      "metadata": { "id": "cell-load-data" },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "\n",
        "# Read CSV â€” iso-8859-1 encoding required for BBC dataset\n",
        "# with_row_index adds a sequential ID column starting at 1\n",
        "articles = pl.read_csv(\n",
        "    \"archive/articles.csv\",\n",
        "    encoding=\"iso-8859-1\"\n",
        ").with_row_index(offset=1)\n",
        "\n",
        "print(f\"Total articles: {len(articles)}\")\n",
        "print(f\"Columns       : {articles.columns}\")\n",
        "print()\n",
        "articles.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-step4-md",
      "metadata": { "id": "cell-step4-md" },
      "source": [
        "## Step 4 â€” Limit Dataset and Set Up Embedding Function\n",
        "\n",
        "**Why limit to 50 articles?**  \n",
        "Each article requires one OpenAI API call for embedding. At 50 articles, this costs fractions of a cent and runs in ~5 seconds. The full BBC dataset has 2000+ articles â€” you'd index all of them in production.\n",
        "\n",
        "**What is `OpenAIEmbeddingFunction`?**  \n",
        "This is ChromaDB's wrapper around the OpenAI embedding API. Instead of calling `openai.embeddings.create()` manually, ChromaDB calls this wrapper automatically whenever you `.add()` documents or `.query()` the collection. You register it once at collection creation and ChromaDB handles the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-embedding-setup",
      "metadata": { "id": "cell-embedding-setup" },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "import chromadb.utils.embedding_functions as embedding_functions\n",
        "\n",
        "# Limit to first 50 articles for this demo\n",
        "N = 50\n",
        "articles = articles.head(N)\n",
        "print(f\"Working with {len(articles)} articles\")\n",
        "\n",
        "# Set up the OpenAI embedding function\n",
        "# model: text-embedding-3-small â†’ 1536-dimensional vectors\n",
        "# This wrapper is called automatically by ChromaDB on .add() and .query()\n",
        "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    model_name=\"text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "print(\"Embedding function ready: text-embedding-3-small (1536 dims)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-step5-md",
      "metadata": { "id": "cell-step5-md" },
      "source": [
        "## Step 5 â€” Generate Embeddings Manually (Optional Preview)\n",
        "\n",
        "This step calls the OpenAI API directly to generate embeddings **before** adding to ChromaDB. This is useful to:\n",
        "- Inspect what a vector looks like (1536 floats)\n",
        "- Understand what happens inside ChromaDB's `.add()` call\n",
        "- Pre-generate embeddings if you want to cache them\n",
        "\n",
        "**Note:** In Step 6, ChromaDB can call `openai_ef` automatically â€” you don't need to pre-generate embeddings unless you want to inspect or cache them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-gen-embeddings",
      "metadata": { "id": "cell-gen-embeddings" },
      "outputs": [],
      "source": [
        "# Extract article text as a Python list\n",
        "articles_list = articles[\"article\"].to_list()\n",
        "\n",
        "# Generate embeddings for all 50 articles via OpenAI API\n",
        "# openai_ef accepts a list of strings and returns a list of vectors\n",
        "print(\"Generating embeddings via OpenAI API...\")\n",
        "vectors = openai_ef(articles_list)\n",
        "\n",
        "# Inspect the result\n",
        "print(f\"Number of vectors  : {len(vectors)}\")\n",
        "print(f\"Dimensions per vector: {len(vectors[0])}\")\n",
        "print(f\"First 5 values of vector[0]: {vectors[0][:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-step6-md",
      "metadata": { "id": "cell-step6-md" },
      "source": [
        "## Step 6 â€” Create ChromaDB Collection and Add Articles\n",
        "\n",
        "**In-memory client vs Persistent client:**\n",
        "```python\n",
        "chromadb.Client()                        # in-memory â€” lost when session ends\n",
        "chromadb.PersistentClient(path=\"./db\")   # saved to disk â€” survives restarts\n",
        "```\n",
        "We use in-memory here. Step 8 shows how to switch to persistent.\n",
        "\n",
        "**Why `get_or_create_collection`?**  \n",
        "Unlike `create_collection`, this does not error if the collection already exists. Safe to re-run without resetting your Colab session.\n",
        "\n",
        "**`hnsw:space: cosine`** â€” tells ChromaDB to measure similarity using cosine distance (angle between vectors). Best for text embeddings where direction matters more than magnitude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-create-collection",
      "metadata": { "id": "cell-create-collection" },
      "outputs": [],
      "source": [
        "# In-memory ChromaDB client (data lives for this session only)\n",
        "client = chromadb.Client()\n",
        "\n",
        "# Create (or open) the articles collection\n",
        "# embedding_function=openai_ef tells ChromaDB how to embed text for queries\n",
        "articles_collection = client.get_or_create_collection(\n",
        "    name=\"articles\",\n",
        "    embedding_function=openai_ef,\n",
        "    metadata={\"hnsw:space\": \"cosine\"}\n",
        ")\n",
        "\n",
        "# Generate IDs â€” ChromaDB requires a unique string ID per document\n",
        "# We use the row index from Polars: ID1, ID2, ID3, ...\n",
        "ids = [f\"ID{x}\" for x in articles[\"index\"].to_list()]\n",
        "\n",
        "# Add all articles with their pre-generated embeddings\n",
        "# Since we already called openai_ef above, we pass embeddings directly\n",
        "# to avoid paying for a second API call\n",
        "articles_collection.add(\n",
        "    documents=articles_list,   # raw text (stored alongside vectors)\n",
        "    ids=ids,                   # unique IDs\n",
        "    embeddings=vectors         # pre-generated 1536-dim vectors\n",
        ")\n",
        "\n",
        "print(f\"Collection name    : {articles_collection.name}\")\n",
        "print(f\"Documents indexed  : {articles_collection.count()}\")\n",
        "print(f\"Sample IDs         : {ids[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-step7-md",
      "metadata": { "id": "cell-step7-md" },
      "source": [
        "## Step 7 â€” Query the Collection\n",
        "\n",
        "**Two ways to query:**\n",
        "\n",
        "```python\n",
        "# Option A â€” query_texts (ChromaDB embeds the query automatically)\n",
        "results = collection.query(query_texts=[\"your question\"], n_results=3)\n",
        "\n",
        "# Option B â€” query_embeddings (you embed the query manually first)\n",
        "query_vec = openai_ef([\"your question\"])\n",
        "results = collection.query(query_embeddings=query_vec, n_results=3)\n",
        "```\n",
        "\n",
        "Option B is used here (same as the video) to show the intermediate embedding step explicitly. Both produce identical results.\n",
        "\n",
        "**What `results` contains:**\n",
        "- `results[\"ids\"]` â€” list of matched document IDs\n",
        "- `results[\"documents\"]` â€” the original article text\n",
        "- `results[\"distances\"]` â€” cosine distance (lower = more similar)\n",
        "\n",
        "**Distance â†’ Similarity:** `similarity = 1.0 - distance`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-query1",
      "metadata": { "id": "cell-query1" },
      "outputs": [],
      "source": [
        "# â”€â”€ Query 1: Exact keyword-style query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "query_1 = \"public transport fares by 7%\"\n",
        "\n",
        "# Embed the query using the same OpenAI model used at index time\n",
        "# CRITICAL: query and documents MUST use the same embedding model\n",
        "# Mixing models produces meaningless similarity scores\n",
        "query_1_vec = openai_ef([query_1])\n",
        "\n",
        "results_1 = articles_collection.query(\n",
        "    query_embeddings=query_1_vec,\n",
        "    n_results=3\n",
        ")\n",
        "\n",
        "print(f\"Query: '{query_1}'\")\n",
        "print(f\"{'â”€'*60}\")\n",
        "for rank, (doc_id, doc, dist) in enumerate(\n",
        "    zip(results_1[\"ids\"][0], results_1[\"documents\"][0], results_1[\"distances\"][0]), 1\n",
        "):\n",
        "    similarity = 1.0 - dist\n",
        "    print(f\"[Rank {rank}] {doc_id} | Similarity: {similarity:.3f}\")\n",
        "    print(f\"  {doc[:150]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-query2",
      "metadata": { "id": "cell-query2" },
      "outputs": [],
      "source": [
        "# â”€â”€ Query 2: Natural language / semantic query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Notice: no exact keyword overlap with articles â€” pure semantic retrieval\n",
        "query_2 = \"What's the deal with the price of petrol?\"\n",
        "\n",
        "query_2_vec = openai_ef([query_2])\n",
        "\n",
        "results_2 = articles_collection.query(\n",
        "    query_embeddings=query_2_vec,\n",
        "    n_results=2\n",
        ")\n",
        "\n",
        "print(f\"Query: '{query_2}'\")\n",
        "print(f\"{'â”€'*60}\")\n",
        "for rank, (doc_id, doc, dist) in enumerate(\n",
        "    zip(results_2[\"ids\"][0], results_2[\"documents\"][0], results_2[\"distances\"][0]), 1\n",
        "):\n",
        "    similarity = 1.0 - dist\n",
        "    print(f\"[Rank {rank}] {doc_id} | Similarity: {similarity:.3f}\")\n",
        "    print(f\"  {doc[:150]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-query3",
      "metadata": { "id": "cell-query3" },
      "outputs": [],
      "source": [
        "# â”€â”€ Query 3: Try your own query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Change this to anything â€” sports, politics, economy, technology\n",
        "your_query = \"football premier league results\"\n",
        "\n",
        "your_vec = openai_ef([your_query])\n",
        "your_results = articles_collection.query(\n",
        "    query_embeddings=your_vec,\n",
        "    n_results=3\n",
        ")\n",
        "\n",
        "print(f\"Query: '{your_query}'\")\n",
        "print(f\"{'â”€'*60}\")\n",
        "for rank, (doc_id, doc, dist) in enumerate(\n",
        "    zip(your_results[\"ids\"][0], your_results[\"documents\"][0], your_results[\"distances\"][0]), 1\n",
        "):\n",
        "    similarity = 1.0 - dist\n",
        "    print(f\"[Rank {rank}] {doc_id} | Similarity: {similarity:.3f}\")\n",
        "    print(f\"  {doc[:150]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-step8-md",
      "metadata": { "id": "cell-step8-md" },
      "source": [
        "## Step 8 â€” Persistent Storage\n",
        "\n",
        "**Problem with `chromadb.Client()`:**  \n",
        "The in-memory client loses all data when the Colab session ends. Every restart requires re-embedding all articles (API cost + time).\n",
        "\n",
        "**Solution â€” `PersistentClient`:**  \n",
        "Saves the HNSW index and all vectors to disk at the specified path. On restart, load the same client and your collection is immediately available â€” no re-embedding needed.\n",
        "\n",
        "```\n",
        "VectorDB/\n",
        "  â””â”€â”€ chroma.sqlite3       â† stores metadata, document text, IDs\n",
        "  â””â”€â”€ <collection-uuid>/\n",
        "        â””â”€â”€ data_level0.bin  â† the HNSW graph (the actual vectors)\n",
        "```\n",
        "\n",
        "In Colab, mount Google Drive first to make the storage permanent across sessions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-persistent",
      "metadata": { "id": "cell-persistent" },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# â”€â”€ Option A: Save to local Colab disk (lost when session ends) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "persistent_client = chromadb.PersistentClient(path=\"VectorDB\")\n",
        "\n",
        "# Create the collection in the persistent client\n",
        "persistent_collection = persistent_client.get_or_create_collection(\n",
        "    name=\"articles_persistent\",\n",
        "    embedding_function=openai_ef,\n",
        "    metadata={\"hnsw:space\": \"cosine\"}\n",
        ")\n",
        "\n",
        "# Add documents (only if not already indexed)\n",
        "if persistent_collection.count() == 0:\n",
        "    persistent_collection.add(\n",
        "        documents=articles_list,\n",
        "        ids=ids,\n",
        "        embeddings=vectors    # reuse pre-generated vectors â€” no new API call\n",
        "    )\n",
        "    print(f\"Indexed {persistent_collection.count()} articles to disk\")\n",
        "else:\n",
        "    print(f\"Collection already has {persistent_collection.count()} articles â€” skipping re-indexing\")\n",
        "\n",
        "# Verify files were created on disk\n",
        "print(\"\\nFiles saved to disk:\")\n",
        "for root, dirs, files in os.walk(\"VectorDB\"):\n",
        "    for f in files:\n",
        "        fpath = os.path.join(root, f)\n",
        "        size_kb = os.path.getsize(fpath) / 1024\n",
        "        print(f\"  {fpath}  ({size_kb:.1f} KB)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-persistent-query",
      "metadata": { "id": "cell-persistent-query" },
      "outputs": [],
      "source": [
        "# â”€â”€ Verify persistent collection works by querying it â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "test_query = \"economic growth inflation rates\"\n",
        "test_vec   = openai_ef([test_query])\n",
        "\n",
        "test_results = persistent_collection.query(\n",
        "    query_embeddings=test_vec,\n",
        "    n_results=2\n",
        ")\n",
        "\n",
        "print(f\"Query from persistent collection: '{test_query}'\")\n",
        "print(f\"{'â”€'*60}\")\n",
        "for rank, (doc_id, doc, dist) in enumerate(\n",
        "    zip(test_results[\"ids\"][0], test_results[\"documents\"][0], test_results[\"distances\"][0]), 1\n",
        "):\n",
        "    similarity = 1.0 - dist\n",
        "    print(f\"[Rank {rank}] {doc_id} | Similarity: {similarity:.3f}\")\n",
        "    print(f\"  {doc[:150]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-summary-md",
      "metadata": { "id": "cell-summary-md" },
      "source": [
        "## Summary\n",
        "\n",
        "| Step | What Happened | Key Concept |\n",
        "|---|---|---|\n",
        "| Step 3 | Loaded CSV with Polars | `with_row_index` adds IDs |\n",
        "| Step 4 | Set up OpenAI embedding function | Wrapper registered with ChromaDB |\n",
        "| Step 5 | Generated 1536-dim vectors via API | Text â†’ numbers representing meaning |\n",
        "| Step 6 | Added articles + vectors to ChromaDB | HNSW graph index built |\n",
        "| Step 7 | Queried with semantic questions | Cosine distance â†’ similarity score |\n",
        "| Step 8 | Saved to disk with PersistentClient | Survives session restarts |\n",
        "\n",
        "---\n",
        "\n",
        "### Key Takeaway\n",
        "Query 2 (`\"What's the deal with the price of petrol?\"`) contains **zero exact words** that appear in any article about fuel prices â€” yet ChromaDB returns the correct articles. This is semantic search working â€” the query vector and the relevant article vectors point in the same direction in 1536-dimensional space because the embedding model learned that *petrol*, *fuel*, *oil prices*, and *price of petrol* are semantically related concepts."
      ]
    }
  ]
}
